def is_chinese(text):
    """Check if the text contains Chinese characters"""
    for char in text:
        if '\u4e00' <= char <= '\u9fff':
            return True
    return False

def post_prompt_func(question: str, question_type: str, evaluator: str, evaluator_kwargs: dict, **kwargs):
    # Check if input is Chinese
    is_chinese_input = is_chinese(question)
    
    if evaluator == "key_items_matching":
        return ""
    
    if is_chinese_input:
        # Chinese version
        base_prompt = "**请以以下格式结束回答：** `最终答案：`, "
        if evaluator == "choices_matching":
            return base_prompt + "后跟一个或多个用逗号分隔的表示正确答案选项的字母。\n**格式示例：** `最终答案：A`"
        elif evaluator == "ordered_list_matching":
            return base_prompt + "后跟一个用逗号分隔的列表。\n**格式示例：** `最终答案：A, B, C, D, E`"
        elif evaluator == "number_matching":
            return base_prompt + "后跟一个数字。\n**格式示例：** `最终答案：123`"
        else:
            return base_prompt + "后跟一个表示正确答案的字符串。"
    else:
        # English version (original)
        base_prompt = "**Finalize your output with:** `Final Answer:`, "
        if evaluator == "choices_matching":
            return base_prompt + "followed by a letter or a comma-separated list of letters which means the option of correct answer.\n**Format example:** `Final Answer: A`"
        elif evaluator == "ordered_list_matching" and all(len(item) == 1 for item in evaluator_kwargs["order"]): # only add uniform suffix for char items
            return base_prompt + "followed by a comma-separated list.\n**Format example:** `Final Answer: A, B, C, D, E`"
        elif evaluator == "number_matching":
            return base_prompt + "followed by a number.\n**Format example:** `Final Answer: 123`"
        else:
            return base_prompt + "followed by a string representing the correct answer."




LLM_EVAL_PROMPT = """# Task Overview

Your task is to evaluate if the whole model answer is semantically consistent with the ground truth answer for the same question. You will be given three pieces of information:

* Question: The question that was asked.
* Ground Truth Answer: The ideal, correct answer.
* Model Answer: The answer generated by an AI model.

Based on your evaluation, you will first provide a brief reason for your decision and then output a simple judgment: `1` for consistent or `0` for inconsistent.

---

# Evaluation Criteria

Determine if the `model answer` conveys the same essential information as the `ground truth answer`.

## When to Judge as Consistent (Judgement: 1)

An answer should be considered consistent if it is factually correct and complete according to the ground truth, even if the phrasing or format differs. This includes cases where:

* The meaning is identical: The answers use different words or sentence structures but mean the same thing.
    * *Example:*  GT: "Both statements are true" vs. Model: "True, True"
    * Reasoning: The model's answer "True, True" is a formatted but semantically identical representation of the ground truth "Both statements are true".
    * Judgement: 1
* The values are mathematically equivalent: The answers express numbers or units differently but are accurately equal.
    * *Example:* GT: "2.25" vs. Model: "\frac{{18}}{{8}}"; GT: "`{{9}}:{{4}}`" vs. Model: "`2.25`".
    * Reasoning: The model's answer, the fraction 9:4
, is mathematically equal to the ground truth decimal 2.25.
    * Judgement: 1
* There are minor variations in names or spelling: The answer refers to the same entity using a common variation.
    * *Example:* GT: "The U.S.S.R." vs. Model: "Soviet Union".
    * Reasoning: "Soviet Union" is a widely accepted and equivalent name for "The U.S.S.R.", referring to the exact same entity.
    * Judgement: 1
* The answer is semantically equivalent for the question: The answers use different words or sentence structures but mean the same thing.
    * *Example:* Question: "What is the medium of The Harbaville Triptych?" GT: "carved ivory" vs. Model: "ivory"
    * Reasoning: The model's answer "ivory" is semantically equivalent to the ground truth "carved ivory" for the question "What is the medium of The Harbaville Triptych?".
    * Judgement: 1

## When to Judge as Inconsistent (Judgement: 0)

An answer should be considered inconsistent if it is factually incorrect, incomplete, or contradictory. This includes cases where:

* It contains a factual error: The model's answer is demonstrably false.
    * *Example:* GT: "The capital of Australia is Canberra." vs. Model: "The capital of Australia is Sydney."
    * Reasoning: The model's answer contains a factual error by incorrectly identifying the capital as "Sydney" when the ground truth is "Canberra".
    * Judgement: 0
* It is missing crucial information: The model's answer omits a key part of the ground truth.
    * *Example:* GT: "The primary colors are red, yellow, and blue." vs. Model: "The primary colors are red and yellow."
    * Reasoning: The model's answer is incomplete because it omits the color "blue," which is a required component of the ground truth list.
    * Judgement: 0
* It contradicts the ground truth: The model's answer states the opposite of the ground truth.
    * *Example:* GT: "The reaction is endothermic." vs. Model: "The reaction is exothermic."
    * Reasoning: The model's answer "exothermic" is the direct opposite of and contradicts the ground truth "endothermic".
    * Judgement: 0
* It is irrelevant: The model's answer is off-topic or fails to address the question.
    * *Example:* GT: "The capital of France is Paris." vs. Model: "The capital of China is Beijing."
    * Reasoning: The model's answer is irrelevant because it provides the capital of China, while the ground truth is about the capital of France.
    * Judgement: 0
* The values are mathematically approximate but inequivalent: The answers are not accurately equal unless approximate is specified.
    * *Example:* GT: "54" vs. Model: "54.584".
    * Reasoning: The model's answer, 54.584, is not accurately equal to the ground truth 54.
    * Judgement: 0
---

# Output Format

Your response must consist of two parts in the following order:

1.  Reasoning: Your explanation for your decision.
2.  Judgement: The numerical judgment on a new line, starting with "Judgement: "

[Question]: {{question}}
[Ground Truth]: {{answer}}
[Model Response] : {{extracted_answer}}
"""
